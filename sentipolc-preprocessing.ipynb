{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/SENTIPOLC Sentiment Polarity Classification - Evalita 2016.csv'\n",
    "test_file = 'data/test_set_sentipolc16_gold2000.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idtwitter</th>\n",
       "      <th>subj</th>\n",
       "      <th>opos</th>\n",
       "      <th>oneg</th>\n",
       "      <th>iro</th>\n",
       "      <th>lpos</th>\n",
       "      <th>lneg</th>\n",
       "      <th>top</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>507074506880712705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Tra 5 minuti presentazione piano scuola del go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>507075789456961536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>@matteorenzi: Alle 10 appuntamento su http://t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507077511902425088</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#labuonascuola gli #evangelisti #digitali non ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>507079183315787777</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Riforma scuola Tutto il discorso di  Renzi su ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>507080190225563648</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>.@matteorenzi @MiurSocial #labuonascuola basta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            idtwitter  subj  opos  oneg  iro  lpos  lneg  top  \\\n",
       "0  507074506880712705     0     0     0    0     0     0    2   \n",
       "1  507075789456961536     1     1     0    0     1     0    2   \n",
       "2  507077511902425088     1     0     1    0     0     1    2   \n",
       "3  507079183315787777     0     0     0    0     0     0    2   \n",
       "4  507080190225563648     1     0     0    0     0     0    2   \n",
       "\n",
       "                                                text  \n",
       "0  Tra 5 minuti presentazione piano scuola del go...  \n",
       "1  @matteorenzi: Alle 10 appuntamento su http://t...  \n",
       "2  #labuonascuola gli #evangelisti #digitali non ...  \n",
       "3  Riforma scuola Tutto il discorso di  Renzi su ...  \n",
       "4  .@matteorenzi @MiurSocial #labuonascuola basta...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(test_file)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.6795\n",
       "1    0.3205\n",
       "Name: lneg, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.lneg.value_counts() / df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idtwitter</th>\n",
       "      <th>subj</th>\n",
       "      <th>opos</th>\n",
       "      <th>oneg</th>\n",
       "      <th>iro</th>\n",
       "      <th>lpos</th>\n",
       "      <th>lneg</th>\n",
       "      <th>top</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122449983151669248</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Intanto la partita per Via Nazionale si compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125485104863780865</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False illusioni, sgradevoli realtà Mario Monti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125513454315507712</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False illusioni, sgradevoli realtà #editoriale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125524238290522113</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mario Monti: Berlusconi risparmi all'Italia il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125527933224886272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mario Monti: Berlusconi risparmi all'Italia il...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            idtwitter  subj  opos  oneg  iro  lpos  lneg  top  \\\n",
       "0  122449983151669248     1     0     1    0     0     1    1   \n",
       "1  125485104863780865     1     0     1    0     0     1    1   \n",
       "2  125513454315507712     1     0     1    0     0     1    1   \n",
       "3  125524238290522113     1     0     1    0     0     1    1   \n",
       "4  125527933224886272     1     0     1    0     0     1    1   \n",
       "\n",
       "                                                text  \n",
       "0  Intanto la partita per Via Nazionale si compli...  \n",
       "1  False illusioni, sgradevoli realtà Mario Monti...  \n",
       "2  False illusioni, sgradevoli realtà #editoriale...  \n",
       "3  Mario Monti: Berlusconi risparmi all'Italia il...  \n",
       "4  Mario Monti: Berlusconi risparmi all'Italia il...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_file)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the distribution of negative and positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.653711\n",
       "1    0.346289\n",
       "Name: lneg, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.lneg.value_counts() / df_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of negative and positive tweets is equally distributed for the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['text', 'lpos']\n",
    "train_data_raw = df_train[columns].as_matrix()\n",
    "test_data_raw = df_test[columns].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Intanto la partita per Via Nazionale si complica. #Saccomanni dice che \"mica tutti sono Mario #Monti\" http://t.co/xPtNz4X7 via @linkiesta'\n",
      "  0]\n",
      " ['False illusioni, sgradevoli realtà Mario Monti http://t.co/WOmMCITs via @AddToAny'\n",
      "  0]]\n",
      "[['Tra 5 minuti presentazione piano scuola del governo #Renzi. #passodopopasso #labuonascuola Stay tuned'\n",
      "  0]\n",
      " [\"@matteorenzi: Alle 10 appuntamento su http://t.co/YphnXknDML #italiariparte #labuonascuola'  #Grandinsegnanti ... #Buonlavoro\"\n",
      "  1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_raw[:2])\n",
    "print(test_data_raw[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We preprocess the tweets with [p/processor](https://github.com/s/preprocessor#available-options).\n",
    "We replace URL, MENTION, HASHTAG, EMOJi, and NUMBER with keywords.\n",
    "The list of positive and negative emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "\n",
    "# check the options at https://github.com/s/preprocessor#available-options\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.EMOJI, p.OPT.NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiley_pos = '$SMILEY_POS$'\n",
    "smiley_neg = '$SMILEY_NEG$'\n",
    "\n",
    "POSITIVE = {\"*O\", \"*-*\", \"*O*\", \"*o*\", \"* *\",\n",
    "            \":P\", \":D\", \":d\", \":p\",\n",
    "            \";P\", \";D\", \";d\", \";p\",\n",
    "            \":-)\", \";-)\", \":=)\", \";=)\",\n",
    "            \":<)\", \":>)\", \";>)\", \";=)\",\n",
    "            \"=}\", \":)\", \"(:;)\",\n",
    "            \"(;\", \":}\", \"{:\", \";}\",\n",
    "            \"{;:]\",\n",
    "            \"[;\", \":')\", \";')\", \":-3\",\n",
    "            \"{;\", \":]\",\n",
    "            \";-3\", \":-x\", \";-x\", \":-X\",\n",
    "            \";-X\", \":-}\", \";-=}\", \":-]\",\n",
    "            \";-]\", \":-.)\",\n",
    "            \"^_^\", \"^-^\"}\n",
    "\n",
    "NEGATIVE = {\":(\", \";(\", \":'(\",\n",
    "            \"=(\", \"={\", \"):\", \");\",\n",
    "            \")':\", \")';\", \")=\", \"}=\",\n",
    "            \";-{{\", \";-{\", \":-{{\", \":-{\",\n",
    "            \":-(\", \";-(\",\n",
    "            \":,)\", \":'{\",\n",
    "            \"[:\", \";]\"\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    :data a list of tweets with their sentiment\n",
    "    :return the tweets preprocessed and split by space\n",
    "    \"\"\"\n",
    "    for text, lpos in data:\n",
    "        sentence = p.tokenize(text).split(' ')\n",
    "        result = []\n",
    "        for word in sentence:\n",
    "            if word in POSITIVE:\n",
    "                result.append(smiley_pos)\n",
    "            elif word in NEGATIVE:\n",
    "                result.append(smiley_neg)\n",
    "            else:\n",
    "                result.append(word)\n",
    "        \n",
    "        yield [result, lpos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_preproccesed = list(preprocess(train_data_raw))\n",
    "test_data_preprocessed = list(preprocess(test_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['$MENTION$', 'sono', \"piu'\", 'tranquillo', 'ora', '$SMILEY_POS$', 'buona', 'giornata.', 'Cmq', 'ci', 'vorrebbe', 'Mario', 'Monti'], 1]\n",
      "[['È', 'online', 'il', 'rapporto', 'del', 'governo', '$HASHTAG$.', 'Si', 'trova', 'a', 'questo', 'link:', '$URL$.', '$HASHTAG$!', '$HASHTAG$', '$MENTION$'], 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_preproccesed[18])\n",
    "print(test_data_preprocessed[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper to find the tweet with positive and/or negative smiley. From the result we can see that there aren't so many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive 228\n",
      "number of negative 25\n"
     ]
    }
   ],
   "source": [
    "def find_smiley(data, pos_or_neg):\n",
    "    idx = []\n",
    "    for i, x in enumerate(data):\n",
    "        if len(set(pos_or_neg).intersection(set(x[0].split()))) > 0:\n",
    "            idx.append(i)\n",
    "    return idx\n",
    "\n",
    "idx_pos = find_smiley(train_data_raw, POSITIVE)\n",
    "idx_neg = find_smiley(train_data_raw, NEGATIVE)\n",
    "\n",
    "print('number of positive {}'.format(len(idx_pos)))\n",
    "print('number of negative {}'.format(len(idx_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$MENTION$', 'sono', \"piu'\", 'tranquillo', 'ora', '$SMILEY_POS$', 'buona', 'giornata.', 'Cmq', 'ci', 'vorrebbe', 'Mario', 'Monti'] 1\n",
      "['Mario', '$HASHTAG$:', 'La', 'lira', 'non', 'era', 'una', 'moneta', 'strana,', 'ma', 'era', 'il', \"più'\", 'delle', 'volte', 'una', 'moneta', 'debole,', 'perche', 'rifletteva', 'caratteristiche', \"dell'Italia\"] 0\n",
      "['Mario', 'Monti', 'a', 'Berlusconi,', \"l'euro\", 'non', 'è', 'in', 'crisi:', 'ROMA,', '$NUMBER$', 'OTT', '?', \"''L'euro\", 'non', 'è', 'in', \"crisi'',\", \"''è\", 'stabile', 'in', 'termin...', '$URL$'] 1\n",
      "['Un', 'parere', 'autorevole', 'e', 'non', 'demagogico', \"sull'euro\", 'da', 'mario', 'monti', '$URL$'] 1\n"
     ]
    }
   ],
   "source": [
    "for tweet, class_value in train_data_preproccesed[18:22]:\n",
    "    print(tweet, class_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([s for s, _ in train_data_preproccesed])\n",
    "y_train = np.array([c for _, c in train_data_preproccesed])\n",
    "\n",
    "x_test = np.array([s for s, _ in test_data_preprocessed])\n",
    "y_test = np.array([c for _, c in test_data_preprocessed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset\n",
    "\n",
    "finally, we save the dataset an np compressed array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('data/sentipolc.npz', x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Intanto', 'la', 'partita', 'per', 'Via', 'Nazionale', 'si', 'complica.', '$HASHTAG$', 'dice', 'che', '\"mica', 'tutti', 'sono', 'Mario', '$HASHTAG$\"', '$URL$', 'via', '$MENTION$']),\n",
       "       list(['False', 'illusioni,', 'sgradevoli', 'realtà', 'Mario', 'Monti', '$URL$', 'via', '$MENTION$']),\n",
       "       list(['False', 'illusioni,', 'sgradevoli', 'realtà', '$HASHTAG$', 'di', 'Mario', 'Monti', 'sul', 'Corriere', 'della', 'Sera:', '$URL$', '$HASHTAG$', 'stampa']),\n",
       "       ...,\n",
       "       list(['$MENTION$', 'Consolati,', 'il', 'governo', '$HASHTAG$', 'ha', 'messo', 'una', 'tassa', 'sulla', 'fortuna.', 'Te', 'non', 'la', 'pagherai', 'mai.', '$MENTION$', 'grazie', 'per', \"l'aiuto\", 'fratello!']),\n",
       "       list(['$MENTION$', 'beh,', 'beate', 'loro!', 'Io', 'nn', 'possiedo', 'nulla', 'di', 'tutto', 'ciò..', 'Devo', 'preoccuparmi?!', '$HASHTAG$']),\n",
       "       list(['Caro', '$HASHTAG$,se', '$HASHTAG$', 'spaccava', 'i', 'computer', 'e', 'ora', 'è', 'il', 'blogger', 'più', 'seguito,forse', 'è', 'più', 'credibile', 'dei', 'politici', 'che', 'rubavano', 'e', 'ancora', 'rubano'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = [' '.join(text) for text in x_train]\n",
    "x_test_text = [' '.join(text) for text in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Dataset into Sequence of IDs per Word\n",
    "\n",
    "prepare the dataset into a sequence of words and generate a dictionary that maps word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer(oov_token=True)\n",
    "word_tokenizer.fit_on_texts(x_train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5616, 'hashtag'),\n",
       " (3227, 'mention'),\n",
       " (2675, 'il'),\n",
       " (2630, 'monti'),\n",
       " (2503, 'di'),\n",
       " (2295, 'url'),\n",
       " (2135, 'governo'),\n",
       " (2098, 'e'),\n",
       " (1873, 'a'),\n",
       " (1828, 'che'),\n",
       " (1795, 'la'),\n",
       " (1527, 'non'),\n",
       " (1326, 'è'),\n",
       " (1272, 'number'),\n",
       " (1133, 'per'),\n",
       " (1088, 'mario'),\n",
       " (1084, 'un'),\n",
       " (1000, 'del'),\n",
       " (961, 'in'),\n",
       " (860, 'i')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_commons = [(v,k) for k, v in word_tokenizer.word_counts.items()]\n",
    "most_commons = sorted(most_commons, key=lambda x: x[0], reverse=True)\n",
    "most_commons[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_path = 'data/sentipolc_word_index.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hashtag', 1),\n",
       " ('mention', 2),\n",
       " ('il', 3),\n",
       " ('monti', 4),\n",
       " ('di', 5),\n",
       " ('url', 6),\n",
       " ('governo', 7),\n",
       " ('e', 8),\n",
       " ('a', 9),\n",
       " ('che', 10)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_tokenizer.word_index.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(word_index_path, 'w') as f:\n",
    "    json.dump(word_tokenizer.word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = word_tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = word_tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Dataset as Encoded Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('data/sentipolc_seq.npz', \n",
    "                    x_train=x_train_seq, y_train=y_train, x_test=x_test_seq, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Dataset into Sequence of IDs per Char\n",
    "\n",
    "prepare the dataset into a sequence of words and generate a dictionary that maps char to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer = Tokenizer(char_level=True, oov_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer.fit_on_texts(x_train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the char dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_index_path = 'data/sentipolc_char_index.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 1),\n",
       " ('i', 2),\n",
       " ('a', 3),\n",
       " ('o', 4),\n",
       " ('e', 5),\n",
       " ('n', 6),\n",
       " ('t', 7),\n",
       " ('r', 8),\n",
       " ('$', 9),\n",
       " ('l', 10)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(char_tokenizer.word_index.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(char_index_path, 'w') as f:\n",
    "    json.dump(char_tokenizer.word_index,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq_char = char_tokenizer.texts_to_sequences(x_train_text)\n",
    "x_test_seq_char = char_tokenizer.texts_to_sequences(x_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('data/sentipolc_char_seq.npz', \n",
    "                    x_train=x_train_seq_char, y_train=y_train, x_test=x_test_seq_char, y_test=y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
